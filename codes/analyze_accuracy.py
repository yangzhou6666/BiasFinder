from sentiment_analysis import SentimentAnalysis
import pandas as pd
from tqdm import tqdm
import os
import random
import numpy as np

seed = 42
random.seed(seed)
np.random.seed(seed)


def check_bias(results, alpha):
    '''decide whether it's bias given prediction results of mutants'''
    is_bias = False
    length = len(results)

    if length == 1:
        # no mutants
        pass
    else:
        mid = int((length - 1) / 2)
        male_results = results[1:mid+1]
        female_results = results[mid+1:]

        assert(len(male_results) == len(female_results))

        pos_M = 1.0 * sum(male_results) / len(male_results)
        pos_F = 1.0 * sum(female_results) / len(female_results)
        ### verify property (2) |pos_M - pos_F| < alpha
        is_bias = False if abs(pos_M - pos_F) < alpha else True

    return is_bias

def predict_on_mutants(df, mutant_dir, sa_system, path_to_result):
    '''
    Given `df`, the dataframe containing original test data
    The function goes to `path_to_mutant`, which stores pre-generated mutants
    Then it use `sa_system` to predict sentiments of mutants
    and store results in `path_to_result`
    '''

    with open(path_to_result, 'w') as f:
        for index, row in tqdm(df.iterrows(), desc="Evaluate"):
            label = row["label"]
            # sentiment = row["sentiment"]
            # if sentiment >= 0.5:
            #     label = 1
            # else: 
            #     label = 0
            text = row["sentence"] # original text
            path_to_mutant = mutant_dir + str(index) + '.csv'
            mutants = [text]
            if os.path.exists(path_to_mutant):
                # if there are generated mutants
                df_mutant = pd.read_csv(path_to_mutant, names=["label", "sentence", "template"], sep="\t")
                for index_new, row_new in df_mutant.iterrows():
                    mutants.append(row_new["sentence"])
            results = []
            results = sa_system.predict_batch(mutants)

            is_bias = check_bias(results, alpha=0.001)

            to_write = str(index) + ',' + str(label) + ',' + str(results[0]) + ',' + str(is_bias) + '\n'
            # each line in this file
            # index, true label, results of original text, is_bias

            f.write(to_write)

def analyze_performance(path_to_result, mutant_dir):
    '''
    Given `path_to_result`, which stores the file generated by predict_on_mutants(...)
    analyze the accuracy of biased/total predictions.
    '''

    with open(path_to_result, 'r') as f:
        lines = f.readlines()
        no_gender_count = 0
        no_gender_correct_count = 0
        total_count = len(lines)
        total_correct_count = 0
        fair_correct_count = 0
        fair_count = 0
        biased_count = 0
        biased_and_correct_count = 0
        for line in lines:
            index = line.split(",")[0]
            true_label = line.split(',')[1]
            pred_label = line.split(',')[2]
            is_bias = line.split(',')[3].strip()
            path_to_mutant = mutant_dir + str(index) + '.csv'

            if true_label == pred_label:
                    total_correct_count += 1
            mutants = []
            if os.path.exists(path_to_mutant):
            #     df_mutant = pd.read_csv(path_to_mutant, names=["label", "sentence", "template"], sep="\t")
            #     for index_new, row_new in df_mutant.iterrows():
            #         mutants.append(row_new["sentence"])
            # if len(mutants) > 3:
                if is_bias == "False":
                    fair_count += 1
                    if true_label == pred_label:
                        fair_correct_count += 1

                if is_bias == 'True':
                    biased_count += 1
                    if true_label == pred_label:
                        biased_and_correct_count += 1
            else:
                no_gender_count += 1
                if true_label == pred_label:
                    no_gender_correct_count += 1

        print("--------**************--------")
        print("Correct Predictions: ", total_correct_count)
        print("Total Predictions: ", total_count)
        print("Accuracy: ", 1.0 * total_correct_count / total_count)
        # print("--------**************--------")
        # print("no gender word---Correct Predictions: ", no_gender_correct_count)
        # print("Total Predictions: ", no_gender_count)
        # print("Accuracy: ", 1.0 * no_gender_correct_count / no_gender_count)
        print("--------**************--------")
        print("with gender word--fair----Correct Predictions: ", fair_correct_count)
        print("Total Predictions: ", fair_count)
        print("Accuracy: ", 1.0 * fair_correct_count / fair_count)
        print("--------**************--------")
        print("Correct and Biased Predictions: ", biased_and_correct_count)
        print("Total Biased Predictions: ", biased_count)
        print("Accuracy on Biased Predictions: ", 1.0 * biased_and_correct_count / biased_count)

def minority_or_majority(mutants, majority):
    
    results = []
    for m in mutants:
        results.append(sa_system.predict(m))

    freq_1 = 0
    freq_0 = 0
    for result in results:
        if result == 1:
            freq_1 += 1
        else:
            freq_0 += 1
    
    if freq_1 > freq_0:
        majority_result = 1
        minority_result = 0
    else:
        majority_result = 0
        minority_result = 1
    
    if majority == True:
        predict = majority_result
    else: 
        predict = minority_result
    
    return predict

def analyze_majority_performance(path_to_result, mutant_dir):
    '''
    Given `path_to_result`, which stores the file generated by predict_on_mutants(...)
    analyze the accuracy of biased/total predictions.
    '''

    with open(path_to_result, 'r') as f:
        lines = f.readlines()
        total_count = len(lines)
        total_correct_count = 0
        biased_count = 0
        count = 0
        biased_and_correct_count = 0
        for line in lines:
            index = line.split(",")[0]
            true_label = line.split(',')[1]
            pred_label = line.split(',')[2]
            is_bias = line.split(',')[3].strip()
            path_to_mutant = mutant_dir + str(index) + '.csv'
            if os.path.exists(path_to_mutant):
                # if there are generated mutants
                mutants = []
                df_mutant = pd.read_csv(path_to_mutant, names=["label", "sentence", "mutant"], sep="\t")
                for index_new, row_new in df_mutant.iterrows():
                    mutants.append(row_new["sentence"])

                result = minority_or_majority(mutants, True)
            else:
                result = pred_label
                
            if str(true_label) == str(result):
                total_correct_count += 1

            if is_bias == 'True':
                biased_count += 1
                if str(true_label) == str(result):
                    biased_and_correct_count += 1
            if os.path.exists(path_to_mutant) and str(true_label) == str(result):
                count += 1
        print(count)
        print("--------**************--------")
        print("Correct Predictions: ", total_correct_count)
        print("Total Predictions: ", total_count)
        print("Accuracy: ", 1.0 * total_correct_count / total_count)
        print("--------**************--------")
        print("Correct and Biased Predictions: ", biased_and_correct_count)
        print("Total Biased Predictions: ", biased_count)
        print("Accuracy on Biased Predictions: ", 1.0 * biased_and_correct_count / biased_count)

def analyze_template_performance(path_to_result, mutant_dir):
    '''
    Given `path_to_result`, which stores the file generated by predict_on_mutants(...)
    analyze the accuracy of biased/total predictions.
    '''

    with open(path_to_result, 'r') as f:
        lines = f.readlines()
        total_count = len(lines)
        total_correct_count = 0
        biased_count = 0
        count = 0
        biased_and_correct_count = 0
        for line in lines:
            index = line.split(",")[0]
            true_label = line.split(',')[1]
            pred_label = line.split(',')[2]
            is_bias = line.split(',')[3].strip()
            path_to_mutant = mutant_dir + str(index) + '.csv'
            if os.path.exists(path_to_mutant):
                # if there are generated mutants
                mutants = []
                df_mutant = pd.read_csv(path_to_mutant, names=["label", "sentence", "mutant"], sep="\t")
                for index_new, row_new in df_mutant.iterrows():
                    mutants.append(row_new["sentence"])
                    template_content = row_new["mutant"]
                if len(mutants)<20:
                    total_count -= 1
                    continue
                try:
                    result = sa_system.predict(template_content)
                except:
                    pass
            else:
                result = pred_label
                
            if str(true_label) == str(result):
                total_correct_count += 1

            if is_bias == 'True':
                biased_count += 1
                if str(true_label) == str(result):
                    biased_and_correct_count += 1
            if os.path.exists(path_to_mutant) and str(true_label) == str(result):
                count += 1
        print(count)
        print("--------**************--------")
        print("Correct Predictions: ", total_correct_count)
        print("Total Predictions: ", total_count)
        print("Accuracy: ", 1.0 * total_correct_count / total_count)
        print("--------**************--------")
        print("Correct and Biased Predictions: ", biased_and_correct_count)
        print("Total Biased Predictions: ", biased_count)
        print("Accuracy on Biased Predictions: ", 1.0 * biased_and_correct_count / biased_count)

if __name__ == '__main__':

    ### initialize an SA system
    model_checkpoint='./../models/epoch20.pt'
    bert_config_file='./../models/uncased_L-12_H-768_A-12/bert_config.json'
    vocab_file='./../models/uncased_L-12_H-768_A-12/vocab.txt'

    sa_system = SentimentAnalysis(model_checkpoint=model_checkpoint,
                                bert_config_file=bert_config_file,
                                vocab_file=vocab_file)
    

    mutant_dir = "../data/biasfinder/gender/sst/each3/each/" 
    # the folder that stores generated mutants.
    df = pd.read_csv("../asset/new_sst_test.csv", header = 0, sep=",")
    # df = pd.read_csv("../asset/new_imdb_test.csv", names=["label", "sentence"], sep="\t")
    # original test set

    alpha = 0.001   # specify "tolerance to bias"
    path_to_result = '../result/395_sst_result_' + str(alpha) + ".txt"

    # predict_on_mutants(df, mutant_dir, sa_system, path_to_result)
    # you don't have to call this each time you run

    analyze_performance(path_to_result, mutant_dir)
    # analyze_majority_performance(path_to_result, mutant_dir)